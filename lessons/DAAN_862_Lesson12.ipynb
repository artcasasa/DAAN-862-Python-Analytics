{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa5b1be-f281-4768-959b-9d429121a5d0",
   "metadata": {},
   "source": [
    "# DAAN 862 Lesson 12 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151ab66a-c755-4d8e-9fb1-cdb6dca08f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c87283-fc98-48e2-a3f8-88982d3437b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load \"Assignment_12.txt\" and count how many file names are in it (10 points)\n",
    "with open('Assignment_12.txt', 'r') as file:\n",
    "    text1 = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b867ecdb-ac8e-4c60-8e85-ab94d4886111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total number of file names: 90\n"
     ]
    }
   ],
   "source": [
    "# Split the file names by spaces\n",
    "file_names = text1.split()\n",
    "total_files = len(file_names)\n",
    "print(f\"1. Total number of file names: {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5df29caf-7e10-4244-8699-cfbefea515bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Number of matching file names: 84\n"
     ]
    }
   ],
   "source": [
    "# 2. Identify the pattern of the file names and count matches (20 points)\n",
    "pattern = r\"^[a-z]+_annotate\\d+_\\d+_\\d+\\.txt$\"\n",
    "matching_files = [name for name in file_names if re.match(pattern, name)]\n",
    "matching_count = len(matching_files)\n",
    "print(f\"2. Number of matching file names: {matching_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed74b4d9-fcc7-49db-9bf0-551fd3a3bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Number of non-matching file names: 6\n",
      "Non-matching file names:\n",
      "1. jdm_ann^otate3_120_1.txt\n",
      "2. jdm_anno&tate6_32_2.txt\n",
      "3. jdm_annotat#e8_177_2.txt\n",
      "4. plos_annotat*e1_6_2.txt\n",
      "5. plos_anno%tate5_1375_3.txt\n",
      "6. plos_annot@ate7_1233_2.txt\n"
     ]
    }
   ],
   "source": [
    "# 3. Find out file names that don't match the pattern (20 points)\n",
    "non_matching_files = [name for name in file_names if not re.match(pattern, name)]\n",
    "non_matching_count = len(non_matching_files)\n",
    "\n",
    "print(f\"3. Number of non-matching file names: {non_matching_count}\")\n",
    "print(\"Non-matching file names:\")\n",
    "for idx, name in enumerate(non_matching_files, start=1):\n",
    "    print(f\"{idx}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "354c7725-b884-43dc-b62b-bcc068215b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Word counts in 'arxiv_annotate1_13_1.txt':\n",
      "abstract: 1\n",
      "misc: 20\n",
      "although: 1\n",
      "the: 44\n",
      "internet: 15\n",
      "as: 28\n",
      "level: 6\n",
      "topology: 8\n",
      "has: 4\n",
      "been: 1\n",
      "extensively: 1\n",
      "studied: 1\n",
      "over: 2\n",
      "past: 1\n",
      "few: 2\n",
      "years: 1\n",
      "little: 1\n",
      "is: 8\n",
      "known: 1\n",
      "about: 1\n",
      "details: 1\n",
      "of: 34\n",
      "taxonomy: 6\n",
      "an: 7\n",
      "node: 1\n",
      "can: 2\n",
      "represent: 1\n",
      "a: 19\n",
      "wide: 2\n",
      "variety: 1\n",
      "organizations: 2\n",
      "e: 2\n",
      "g: 2\n",
      "large: 3\n",
      "isp: 2\n",
      "or: 3\n",
      "small: 4\n",
      "private: 2\n",
      "business: 3\n",
      "university: 3\n",
      "with: 9\n",
      "vastly: 1\n",
      "different: 12\n",
      "network: 8\n",
      "characteristics: 2\n",
      "external: 1\n",
      "connectivity: 2\n",
      "patterns: 3\n",
      "growth: 4\n",
      "tendencies: 1\n",
      "and: 24\n",
      "other: 6\n",
      "properties: 2\n",
      "that: 12\n",
      "we: 20\n",
      "hardly: 1\n",
      "neglect: 1\n",
      "while: 1\n",
      "working: 1\n",
      "on: 9\n",
      "veracious: 1\n",
      "representations: 1\n",
      "in: 19\n",
      "simulation: 1\n",
      "environments: 2\n",
      "aimx: 3\n",
      "this: 5\n",
      "paper: 1\n",
      "introduce: 2\n",
      "radically: 2\n",
      "new: 3\n",
      "approach: 3\n",
      "based: 3\n",
      "machine: 3\n",
      "learning: 3\n",
      "techniques: 1\n",
      "to: 18\n",
      "map: 1\n",
      "all: 1\n",
      "ases: 12\n",
      "into: 2\n",
      "natural: 1\n",
      "ownx: 12\n",
      "successfully: 1\n",
      "classify: 4\n",
      "number: 16\n",
      "percent: 3\n",
      "expected: 1\n",
      "accuracy: 2\n",
      "release: 1\n",
      "community: 1\n",
      "dataset: 2\n",
      "augmented: 1\n",
      "information: 2\n",
      "set: 3\n",
      "attributes: 1\n",
      "used: 2\n",
      "believe: 1\n",
      "will: 3\n",
      "serve: 1\n",
      "invaluable: 1\n",
      "addition: 1\n",
      "further: 2\n",
      "understanding: 3\n",
      "structure: 3\n",
      "evolution: 5\n",
      "introduction: 1\n",
      "rapid: 1\n",
      "expansion: 1\n",
      "last: 1\n",
      "two: 1\n",
      "decades: 1\n",
      "produced: 1\n",
      "scale: 1\n",
      "system: 2\n",
      "thousands: 2\n",
      "diverse: 1\n",
      "independently: 1\n",
      "managed: 1\n",
      "networks: 2\n",
      "collectively: 1\n",
      "provide: 1\n",
      "global: 1\n",
      "across: 1\n",
      "spectrum: 1\n",
      "geopolitical: 1\n",
      "from: 6\n",
      "globally: 1\n",
      "routable: 1\n",
      "identifiers: 1\n",
      "increased: 1\n",
      "less: 1\n",
      "than: 2\n",
      "more: 1\n",
      "exerting: 1\n",
      "significant: 1\n",
      "pressure: 1\n",
      "interdomain: 1\n",
      "routing: 2\n",
      "well: 2\n",
      "functional: 1\n",
      "structural: 1\n",
      "parts: 1\n",
      "impressive: 1\n",
      "resulted: 1\n",
      "heterogenous: 1\n",
      "highly: 1\n",
      "complex: 1\n",
      "challenges: 1\n",
      "accurate: 2\n",
      "realistic: 2\n",
      "modeling: 4\n",
      "infrastructure: 1\n",
      "particular: 1\n",
      "intermix: 1\n",
      "owned: 1\n",
      "operated: 1\n",
      "by: 4\n",
      "many: 2\n",
      "backbone: 1\n",
      "providers: 4\n",
      "regional: 1\n",
      "access: 1\n",
      "universities: 1\n",
      "companies: 2\n",
      "statistical: 1\n",
      "faithfully: 1\n",
      "characterizes: 1\n",
      "types: 10\n",
      "critical: 1\n",
      "path: 1\n",
      "toward: 1\n",
      "for: 7\n",
      "its: 3\n",
      "knowledge: 1\n",
      "mandatory: 1\n",
      "augmenting: 1\n",
      "synthetically: 1\n",
      "constructed: 1\n",
      "measured: 1\n",
      "topologies: 3\n",
      "intra: 1\n",
      "inter: 1\n",
      "router: 3\n",
      "example: 3\n",
      "expect: 1\n",
      "dual: 2\n",
      "homed: 2\n",
      "be: 1\n",
      "drastically: 1\n",
      "company: 2\n",
      "likely: 1\n",
      "contain: 1\n",
      "dozens: 1\n",
      "internal: 1\n",
      "routers: 1\n",
      "hosts: 1\n",
      "elements: 1\n",
      "switches: 1\n",
      "servers: 1\n",
      "firewalls: 1\n",
      "hand: 2\n",
      "most: 1\n",
      "probably: 1\n",
      "have: 1\n",
      "single: 1\n",
      "simple: 1\n",
      "since: 2\n",
      "there: 1\n",
      "such: 1\n",
      "diversity: 1\n",
      "among: 1\n",
      "cannot: 2\n",
      "accurately: 1\n",
      "augment: 1\n",
      "appropriate: 1\n",
      "if: 1\n",
      "characterize: 1\n",
      "composing: 1\n",
      "moreover: 1\n",
      "annotating: 1\n",
      "their: 1\n",
      "prerequisite: 1\n",
      "exhibit: 1\n",
      "service: 1\n",
      "grow: 2\n",
      "attracting: 1\n",
      "customers: 1\n",
      "engaging: 1\n",
      "agreements: 1\n",
      "isps: 2\n",
      "connect: 1\n",
      "through: 1\n",
      "one: 1\n",
      "do: 1\n",
      "not: 1\n",
      "significantly: 1\n",
      "time: 1\n",
      "thus: 1\n",
      "categorizing: 1\n",
      "necessary: 2\n",
      "identify: 2\n",
      "develop: 2\n",
      "models: 1\n",
      "also: 1\n",
      "mapping: 1\n",
      "ip: 2\n",
      "addresses: 1\n",
      "users: 2\n",
      "traffic: 1\n",
      "analysis: 1\n",
      "studies: 1\n",
      "often: 1\n",
      "required: 1\n",
      "distinguish: 1\n",
      "between: 3\n",
      "packets: 1\n",
      "come: 1\n",
      "home: 1\n",
      "given: 1\n",
      "possible: 1\n",
      "realize: 1\n",
      "goal: 1\n",
      "checking: 1\n",
      "type: 1\n",
      "originates: 1\n",
      "prefix: 1\n",
      "which: 1\n",
      "address: 1\n",
      "lies: 1\n",
      "work: 2\n",
      "construct: 1\n",
      "representative: 2\n",
      "algorithm: 3\n",
      "empirically: 1\n",
      "observed: 1\n",
      "differences: 3\n",
      "use: 2\n",
      "data: 2\n",
      "registries: 1\n",
      "irr: 1\n",
      "citation: 2\n",
      "routeviews: 1\n",
      "intrinsic: 1\n",
      "then: 1\n",
      "employ: 1\n",
      "novel: 1\n",
      "technique: 1\n",
      "build: 1\n",
      "classification: 3\n",
      "exploits: 1\n",
      "these: 1\n",
      "six: 1\n",
      "classes: 2\n",
      "reflect: 1\n",
      "infrastructures: 1\n",
      "derive: 1\n",
      "macroscopic: 1\n",
      "statistics: 1\n",
      "validate: 2\n",
      "our: 7\n",
      "results: 3\n",
      "using: 1\n",
      "sample: 1\n",
      "manually: 1\n",
      "identified: 1\n",
      "validation: 1\n",
      "demonstrates: 1\n",
      "achieves: 1\n",
      "high: 1\n",
      "examined: 1\n",
      "classifications: 1\n",
      "were: 1\n",
      "correct: 1\n",
      "finally: 1\n",
      "make: 1\n",
      "classifier: 1\n",
      "publicly: 1\n",
      "available: 1\n",
      "promote: 1\n",
      "research: 1\n",
      "s: 1\n",
      "section: 6\n",
      "start: 1\n",
      "brief: 1\n",
      "discussion: 1\n",
      "related: 1\n",
      "describes: 1\n",
      "specify: 1\n",
      "experiments: 1\n",
      "introduces: 1\n",
      "them: 1\n",
      "conclude: 1\n"
     ]
    }
   ],
   "source": [
    "# 4. Normalize and count words in \"arxiv_annotate1_13_1.txt\" (Normalize words and find counts)\n",
    "with open('arxiv_annotate1_13_1.txt', 'r') as file:\n",
    "    text2 = file.read()\n",
    "\n",
    "# Normalize text: convert to lowercase and remove punctuation\n",
    "normalized_text = re.sub(r'[^\\w\\s]', '', text2.lower())\n",
    "word_counts = Counter(normalized_text.split())\n",
    "\n",
    "print(\"4. Word counts in 'arxiv_annotate1_13_1.txt':\")\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96939265-29f3-427c-9a36-3cbd484e004f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
